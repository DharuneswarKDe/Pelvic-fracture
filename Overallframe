"In the first stage, an encoder-decoder
network segments the pelvis into three parts according to its
anatomical structure."


import os
import cv2
import numpy as np
import nibabel as nib
from scipy.spatial import procrustes
from sklearn.decomposition import PCA
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate

# Define directories containing your data
image_dir = r"C:\\Users\\Dharuneswar Kumar\\Downloads\\Mini project\\img\\"
output_dir =r"C:\\Users\\Dharuneswar Kumar\\Downloads\\Mini project\\outputimg"
mask_dir = r"C:\\Users\\Dharuneswar Kumar\\Downloads\\Mini project\\masks"



############BASIC FUNCTIONS#################################################

def check_multiclass_segmentation(segmented_image):
    # Check if the segmented image has more than one channel
    if len(segmented_image.shape) > 2 and segmented_image.shape[2] > 1:
        return True
    else:
        return False
    
def rotate_volume(data):
    # Rotate volume by 90 degrees to the right
    rotated_data = np.rot90(data, k=-1)
    return rotated_data


#############################################

def unet_model(input_shape=(None, None, 1)):
    inputs = Input(input_shape)
    
    # Encoder
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)
    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)
    drop5 = Dropout(0.5)(conv5)

    # Decoder
    up6 = Conv2D(512, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop5))
    merge6 = concatenate([drop4, up6], axis=3)
    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)
    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)

    up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6))
    merge7 = concatenate([conv3, up7], axis=3)
    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)
    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)

    up8 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv7))
    merge8 = concatenate([conv2, up8], axis=3)
    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)
    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)

    up9 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv8))
    merge9 = concatenate([conv1, up9], axis=3)
    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)
    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)
    conv9 = Conv2D(2, 3, activation='relu', padding='same')(conv9)
    
    # Output
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Define function for image segmentation using U-Net model
def encoder_decoder_segmentation(image):
    image = image.astype(np.float32)
    min_val = np.min(image)
    max_val = np.max(image)
    normalized_image = (image - min_val) / (max_val - min_val)
    # Perform segmentation using the trained U-Net model
    # Return the segmented images
    if len(image.shape) == 3 and image.shape[2] == 3:  
        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    elif len(image.shape) == 2:  # Grayscale image
        gray_image = image
    else:
        raise ValueError("Input image must be either grayscale or RGB")

    gray_image = gray_image.astype(np.float32)
    
    if len(normalized_image.shape) != 3 or normalized_image.shape[2] != 1:
        normalized_image = np.expand_dims(normalized_image, axis=-1)
        
    input_shape = image.shape + (1,)
    model = unet_model(input_shape)
    
    image = image / 255.0  # Normalize pixel values to [0, 1]
    
    segmented_image = model.predict(np.expand_dims(image, axis=0))[0]
    nii_image = nib.Nifti1Image(segmented_image, np.eye(4))  # Adjust affine matrix as needed
    plt.figure(figsize=(8, 6))
    plt.imshow(image, cmap='gray')
    plt.title('Original Image')
    plt.imshow(segmented_image, alpha=0.5)
    plt.axis('off')
    plt.show()
            

    binval=check_multiclass_segmentation(segmented_image)
    # Threshold the segmented image to obtain binary masks
    thresholded_image = (segmented_image > 0.5).astype(np.uint8)
    if(binval):
    # Assuming the segmented image contains multiple classes (sacrum, left innominate bone, right innominate bone)
        sacrum_mask = thresholded_image[..., 0]  # Extract sacrum mask
        left_innominate_mask = thresholded_image[..., 1]  # Extract left innominate bone mask
        right_innominate_mask = thresholded_image[..., 2]  # Extract right innominate bone mask
        return [sacrum_mask, left_innominate_mask, right_innominate_mask]
    else:
        print("Error")




# Define function for applying Iterative Closest Point (ICP) algorithm
def apply_icp(source_image, target_image):
    # Implement Iterative Closest Point (ICP) algorithm
    # This function should compute the spatial transformation matrix M
    # and apply it to the source image to align with the target image
    
    M = np.eye(3)  # Placeholder for transformation matrix
    transformed_image = cv2.warpAffine(source_image, M, (source_image.shape[1], source_image.shape[0]))
    
    return transformed_image

# Define function for analyzing symmetric properties and density differences
def analyze_symmetry_and_density(image):
    # Analyze symmetric properties and density differences
    # This function should compare density differences between fractured and normal sides
    # and identify potential fracture areas
    
    # Implement your analysis here
    
    potential_fracture_area = None  # Placeholder for potential fracture area
    
    return potential_fracture_area



# List files in the image directory
image_files = os.listdir(image_dir) 
mask_files = os.listdir(mask_dir)
 # Process each slice individually
for img_file,mask_file in zip(image_files,mask_files):
    img_path = os.path.join(image_dir, img_file)
    mask_path = os.path.join(mask_dir, mask_file)
    # Load the image
    img = nib.load(img_path)
    mask_img = nib.load(mask_path).get_fdata()
    imgdata=img.get_fdata()
    if imgdata is None:
        print(f"Error loading image: {img_path}")
        continue  # Skip to the next image
    # Rotate the volume
    rotated_data = rotate_volume(imgdata) 
    rotated_mask_img = rotate_volume(mask_img)
    # Get depth (number of slices)
    depth = rotated_data.shape[2]
    # Process each slice individually
   
    processed_ct_img = np.zeros_like(rotated_data)
    for i in range(60,80):
        img_slice = rotated_data[:,:,i]
        mask_slice = rotated_mask_img[:,:,i]
        masked_ct_slice = np.where(mask_slice > 0, img_slice, 0)
        processed_ct_img[:,:,i] = masked_ct_slice   
        nii_image = nib.Nifti1Image(processed_ct_img, np.eye(4))  # Adjust affine matrix as needed
    
        # Define the output path for the NIfTI file
        output_ct_path = os.path.join(output_dir, f"processed_{img_file.replace('.nii', '.nii.gz')}") 
        # Save the NIfTI image to a file
        nib.save(nii_image, output_ct_path)      

        # Perform image segmentation using Encoder-Decoder
        segmented_images = encoder_decoder_segmentation(mask_slice)
